{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        yaml.safe_dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"value and filename must be provided\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename=None):\n",
    "    if filename is not None:\n",
    "        yaml.safe_load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"filename must be provided\".capitalize())\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Production Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask=None\n",
    "):\n",
    "    if (\n",
    "        isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(value, torch.Tensor)\n",
    "    ):\n",
    "        assert (\n",
    "            query.size() == key.size() == value.size()\n",
    "        ), \"query, key, and value must have the same size\"\n",
    "\n",
    "        result = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(value.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            lookup = torch.triu(\n",
    "                input=torch.ones_like(mask.unsqueeze(1).unsqueeze(2)), diagonal=1\n",
    "            )\n",
    "            lookup = torch.where(lookup == 1.0, 1e-19, lookup)\n",
    "            result = torch.add(result, lookup)\n",
    "\n",
    "        attention = torch.softmax(result, dim=-1)\n",
    "        attention = torch.matmul(attention, value)\n",
    "\n",
    "        return attention\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"query, key, and value must be torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    scaled = scaled_dot_product_attention(\n",
    "        query=torch.randn(40, 8, 200, 512 // 8),\n",
    "        key=torch.randn(40, 8, 200, 512 // 8),\n",
    "        value=torch.randn(40, 8, 200, 512 // 8),\n",
    "        mask=torch.randn(40, 200,),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        nheads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.dimension % self.nheads == 0, \"dimension must be divisible by nheads\".capitalize()\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=self.bias\n",
    "        )\n",
    "\n",
    "        self.layer = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=self.bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            QKV = self.QKV(x)\n",
    "\n",
    "            self.query, self.key, self.value = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "\n",
    "            assert (\n",
    "                self.query.size() == self.key.size() == self.value.size()\n",
    "            ), \"QKV must have the same size\".capitalize()\n",
    "\n",
    "            self.query = self.query.view(\n",
    "                self.query.size(0),\n",
    "                self.query.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "            self.key = self.key.view(\n",
    "                self.key.size(0),\n",
    "                self.key.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "            self.value = self.value.view(\n",
    "                self.value.size(0),\n",
    "                self.value.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "\n",
    "            self.query = self.query.permute(0, 2, 1, 3)\n",
    "            self.key = self.key.permute(0, 2, 1, 3)\n",
    "            self.value = self.value.permute(0, 2, 1, 3)\n",
    "\n",
    "            self.attention = scaled_dot_product_attention(\n",
    "                query=self.query, key=self.key, value=self.value, mask=mask\n",
    "            )\n",
    "\n",
    "            assert (\n",
    "                self.attention.size()\n",
    "                == self.query.size()\n",
    "                == self.key.size()\n",
    "                == self.value.size()\n",
    "            ), \"Attention must have the same size as QKV\".capitalize()\n",
    "\n",
    "            self.attention = self.attention.view(\n",
    "                self.attention.size(0),\n",
    "                self.attention.size(2),\n",
    "                self.attention.size(1) * self.attention.size(3),\n",
    "            )\n",
    "\n",
    "            return self.layer(self.attention)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attention = MultiHeadAttentionLayer(\n",
    "        dimension=512,\n",
    "        nheads=8,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    )\n",
    "    \n",
    "    print(attention(torch.randn(40, 200, 512)).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 512,\n",
    "        out_features: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            self.activation_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "        elif self.activation == \"gelu\":\n",
    "            self.activation_fn = nn.GELU()\n",
    "\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            self.activation_fn = nn.LeakyReLU(inplace=True, negative_slope=0.2)\n",
    "\n",
    "        self.layers = list()\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=self.in_features,\n",
    "                    out_features=self.out_features,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "            )\n",
    "            if index == 0:\n",
    "                self.layers.append(self.activation_fn)\n",
    "                self.layers.append(nn.Dropout(p=self.dropout))\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = in_features\n",
    "\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.model(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    network = FeedForwardNeuralNetwork(\n",
    "        in_features=512,\n",
    "        out_features=2048,\n",
    "        activation=\"gelu\",\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    )\n",
    "\n",
    "    print(network(torch.randn(40, 200, 512)).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(\n",
    "        self, normalized_shape: int = 512, eps: float = 1e-05, bias: bool = True\n",
    "    ):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.epsilon = eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.gamma = nn.Parameter(data=torch.ones((normalized_shape,)))\n",
    "        self.beta = nn.Parameter(data=torch.zeros((normalized_shape,)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.mean = torch.mean(x, dim=-1)\n",
    "            self.variance = torch.var(x, dim=-1)\n",
    "\n",
    "            self.mean = self.mean.unsqueeze(-1)\n",
    "            self.variance = self.variance.unsqueeze(-1)\n",
    "\n",
    "            normalized = (\n",
    "                self.gamma * (x - self.mean) / torch.sqrt(self.variance + self.epsilon)\n",
    "                + self.beta\n",
    "            )\n",
    "\n",
    "            return normalized\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    layer_norm = LayerNormalization(\n",
    "        normalized_shape=512,\n",
    "        eps=1e-5,\n",
    "        bias=True,\n",
    "    )\n",
    "    \n",
    "    print(layer_norm(torch.randn(40, 200, 512)).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, sequence_length: int = 200, dimension: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dimension = dimension\n",
    "\n",
    "        self.positional_encoding = torch.zeros((self.sequence_length, self.dimension))\n",
    "\n",
    "        for position in range(self.sequence_length):\n",
    "            for index in range(self.dimension):\n",
    "                if index % 2 == 0:\n",
    "                    self.positional_encoding[position, index] = math.sin(\n",
    "                        position / (10000 ** ((2 * index) / dimension))\n",
    "                    )\n",
    "                else:\n",
    "                    self.positional_encoding[position, index] = math.cos(\n",
    "                        position / (10000 ** ((2 * index) / dimension))\n",
    "                    )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.positional_encoding.unsqueeze(0)[:, : x.size(1), :]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    positional_encoding = PositionalEncoding(sequence_length=200, dimension=512)\n",
    "    print(positional_encoding(torch.randn(40, 200, 512)).size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
